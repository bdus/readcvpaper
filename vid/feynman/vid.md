# 视频目标检测

之前写过一点，过了一段时间了，主要参考之前写的，和[detection in the age](../../DET/age_advances)

## 背景

得益于深度学习的兴起，图像目标检测任务在过去几年中取得了巨大进展，检测性能提升。
但在视频监控、车辆辅助驾驶等领域，基于视频的目标检测有着更为广泛的需求。

视频目标检测任务的数据集首先在ImageNet VID中放出，更多的可以看一下[datasets](./datasets.md)

## 挑战与应对

相比与静态目标检测任务（DET），对视频目标检测(VID)来说，
视频中既有单帧的时域信息，还有视频额外的帧间、时域信息
然而视频同时也带来了不同于DET的一些挑战，
比如说：运动模糊、形态变化多样性、光照变化多样性、姿态变化、视频失焦、算法开销和速度等等。

图像的时域信息当然对视频中目标的检测至关重要 但
仅利用图像目标检测技术检测视频中的目标并不能得到很好的检测结果。
如何利用视频中目标时序信息和上下文等信息称为提成视频目标检测性能的关键。

对于视频来讲，相邻帧目标之间存在明显的上下文关系，这种关系
在技术上可以是Tracking、3D卷积、光流、框选择等等。

例如
经典的单目标跟踪算法TLD,通过Tracking-Learning-Detection学习目标的帧间变换，并进行Location。

## 更高更快更强

针对不同的挑战，视频信息的利用方法并不一样
一类方法是要优化算法的开销和速度，因此关注如何利用视频信息来加速视频目标检测这个过程。因为视频相邻帧存在着大量的信息冗余，如果可以用廉价的方法来加速，而不损害性能，在实际应用中很有意义。

另一方法关注上下文信息如何有效地减轻单帧图片检测中由于运动模糊、物体面积过小等导致的困难，从而来提高检测性能。

当然理想就是能又好又快又鲁棒。

# 算法

从2015年ImageNet在目标检测下新增了VID挑战，也是将目标检测带到视频领域成为热点问题的开端。主要思路是结合帧间信息，跟踪信息。其实这里看起来很熟悉，因为有一个很古老的话题，就是运动目标检测的方法就是利用帧差、光流以及图像帧的空域信息。有一些有代表性的视频目标检测方法。

## 跟踪与后处理

>这一系列 T-CNN、 Seq-NMS、 MCMOT、 Context matters

T-CNN的方法结合静态检测信息、上下文信息和跟踪信息，首先通过静态的图片检测出结果，然后用上下文抑制降低监测置信度/分数（score）比较低的部分，利用运动传播（光流信息）降低漏检率，接着结合跟踪信息进行tubelet（管道滤波），用跟踪的值修正检测，最终利用建议框合并或非极大值抑制的方法消除重复框。

Seq-NMS在视频连续帧中沿着高置信度的bounding boxes构建一个序列，和序列相邻的框被抑制。

MCMOT把检测的后处理问题当做了多目标跟踪问题，尝试了许多诸如颜色、置信度，光流等等的手工特征得到后验概率最大值，然后对检测框进行筛选。

// 填坑

## 特征融合

这一系列的工作做的流畅清楚 探索了速度和精度

Deep Feature Flow结合光流的思路，实现特征图的帧间传播和复用。算法首先在关键帧（Key Frame）进行特诊图提取，然后将特征在帧间传播，最后进行特征图映射，然后进行检测

FGFA这个是Deep feature flow的后续工作，ImageNet2017年的冠军。算法的亮点在于对视频中运动模糊的目标检测效果非常好，首先用特征网络从图片中提空间特征，然后用光流网络计算出帧间的光流信息，与第一部分的特征融合得到特征图，然后这样得到每一帧视频的特征图，然后对当前特征图的前后K帧进行加权的特征融合，（这个权值根据结果的置信更新），得到融合特征后进行检测，后接的是目标检测网络，得到结果。整个网络训练是端到端的，降低了模块之间的误差。

// 填坑

## Detect & track 

这一系列的工作结合检测和跟踪 比如Det&Tra、 Det or Tra

// 填坑

# reference

[M] Recent Advances in Object Detection in the Age
